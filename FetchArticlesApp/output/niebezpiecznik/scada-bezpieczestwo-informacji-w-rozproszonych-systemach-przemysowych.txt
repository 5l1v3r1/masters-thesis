Każda informacja w XXI wieku ma cenę. Oczywiście niektóre informacje są warte fortunę, a za niektóre nie dostaniemy nawet pół złamanego grosza. Czy na tą cenę składają się wyłącznie dane? Autorem tego artykułu jest vizzdoom. Kontakt z autorem: vizzdoom at gmail kropka com Wstęp Wartość informacji nie zależy wyłącznie od komunikatu, jaki w sobie ona niesie. Sposób przechowywania (przekaz słowny, pamięć, stan przełączników, dyski twarde), czas przechowywania, jakość (spójność danych, zawartość meta-informacji, dane kontrolne), możliwości replikacji (łatwość powielania danych, stopień zmienności danych), kontrola dostępu do danych… wszystkie te, wraz z dziesiątkami innych czynników sprawiają, że przechowywanie niektórych, nawet bardzo krótkich informacji może być w zasięgu tylko najbogatszych. Przemysł jest nastawiony na tworzenie oraz przetwarzanie dóbr wszelakiego rodzaju. Automatyzacja, kontrola, bezpieczeństwo i niezawodność systemów przemysłowych wymaga analizowania ogromnej liczby danych w określony sposób. Skalowalność wymaga rozbicia infrastruktury, aby znajdowała się w wielu miejscach i dalej ze sobą niezawodnie współpracowała. Rozproszone systemy przemysłowe zyskują ogromną popularność. Przyrost rocznej wartości rynkowej samych systemów kontroli na świecie w roku 2000 był szacowany na 10.3 miliardów dolarów (13.9 miliardów w 2009) [6]. Są to najprostsze przypadki zastosowania tych technologii. To ogromne ilości pieniędzy (informacji), które każda firma powinna chronić. Dokument ten przedstawia podstawowe błędy projektowe, ideowe i techniczne, które mogą skutkować udanymi atakami na Rozproszone Systemy Przemysłowe. W następnych rozdziałach można się dowiedzieć o samej strukturze Rozproszonych Systemów (Distributed Systems), o dużej części wspólnej bezpieczeństwa systemów zwykłych i rozproszonych, niebezpieczeństwie, które niesie ze sobą sieć Internet. Ponadto zostanie przytoczone parę zdań o mediach transmisyjnych oraz atakach polegających na zalewaniu i problemie odmowy usługi. Dużą część tej pracy stanowi analiza problemu uwierzytelniania, autoryzacji, etyka bezpieczeństwa informacji oraz analiza ataków socjotechnicznych. 1. Systemy rozproszone w przemyśle Czym jest system rozproszony? Odpowiedź nie jest jednoznaczna. W literaturze najczęściej pojawia się określenie, że system rozproszony to zbiór niezależnych komputerów, sprawiający na jego użytkownikach wrażenie jednego, logicznie zwartego systemu. Definicja ta oddaje główne założenie, jednakże należy jeszcze określić główne cele tej technologi oraz wspomnieć o aspekcie, dzięki któremu powstały systemy rozproszone w przemyśle. Aspektami tymi są sieci komputerowe oraz sterowniki PLC. Powstanie sieci komputerowych spowodowało gwałtowny rozwój technologiczny. Transfer danych na duże odległości pomiędzy komputerami spowodował decentralizację obliczeń, zadań i magazynów do przechowywania danych. Sieci komputerowe po krótkim czasie stały się dostępne w każdym domu. W przemyśle, podobnie jak w rozwoju systemów komputerowych, kamieniem milowym było stworzenie swobodnie programowalnego sterownika przemysłowego (PLC). PLC jest komputerem o ścisłej specjalizacji, którego cechuje duża niezawodność i analiza danych z wielu źródeł. Automatyzacja wielu procesów spowodowała, że fabryki nawet całkowicie nie związane z rynkiem Nowych Technologi, zaczęły być nimi naszpikowane, dzięki czemu wzniosły się na nowy, lepszy poziom. Złożenie tych dwóch elementów – sieci komputerowych oraz wyspecjalizowanych jednostek obliczeniowych okazało się niezwykle dobrym pomysłem i wywołało wielki ruch związany z tworzeniem systemów rozproszonych. Systemy te w założeniach po pierwsze miały ukrywać przed użytkownikiem wrażenie, że ma on przed sobą strukturę złożoną z wielu, odległych od siebie komputerów. Po drugie, interfejs tego systemu miał gwarantować jednolity i spójny interfejs niezależnie od miejsca interakcji z nim. Ponadto takie rozwiązania miały cechować się dużą skalowalnością. Nie można było przejmować się faktem dołożenia kolejnych komputerów do farmy obliczeniowej, ich architekturą, metodami komunikacji i innymi, podobnymi aspektami. Cele to zostały osiągnięte i dzisiaj systemy rozproszone można spotkać prawie na każdym kroku. System rozproszony musi być Systemem Przezroczystym. Pojęcie to określa: Przezroczystość dostępu – ujednolicanie metod dostępu do danych i ukrywanie różnic w ich reprezentacji Przezroczystość położenia – użytkownicy nie mogą określić fizycznego położenia zasobu Przezroczystość wędrówki – możliwość przenoszenia zasobów między serwerami bez zmiany sposoby odwoływania się do nich Przezroczystość przemieszczania – zasoby mogą być przenoszone nawet podczas ich używania Przezroczystość zwielokrotniania – ukrywanie przed użytkownikami faktu zwielokrotnienia zasobów Przezroczystość współbieżności – możliwość współbieżnego przetwarzania danych nie powodująca powstawanie niespójności Przezroczystość awarii – maskowanie przejściowych awarii poszczególnych komponentów systemu rozproszonego Przezroczystość trwałości – maskowanie sposobu przechowywania zasobu System przemysłowy cechuje dodatkowo pewien ściśle określony czas reakcji na pojawiające się zdarzenia, niezawodność oraz umiejętność rekonfiguracji w momencie awarii. Analiza bezpieczeństwa rozproszonego systemu przemysłowego niczym nie różni się od analizy zwykłego systemu rozproszonego. Większą uwagę należy zwrócić na wspomnianą w poprzednim akapicie niezawodność oraz czas reakcji, lecz porządny administrator (lub audytor) i tak musi zwracać uwagę na te czynniki. Ważnym składnikiem, który rozróżnia te dwa typy systemów jest charakter ich pracy. Systemy przemysłowe są systemami wielkimi, z dużą gamą specjalistycznego sprzętu pracującego w trudniejszych warunkach. Jednak z punktu widzenia informatyki – dalej jest to zbiór komputerów, sterowników, programów, urządzeń wejścia/wyjścia i mediów komunikacyjnych. Przykład przemysłowego systemu rozproszonego przedstawiony przez prof. Andrzeja Kwietnia [8] został zamieszczony na Rysunku 1. oraz Rysunku 2. 2. Ogólna analiza niebezpieczeństwa systemów Każdy system komputerowy musi być odpowiednio zabezpieczony. Każdy moduł jest wystawiony na wiele niebezpieczeństw. Przed analizą systemów rozproszonych i przemysłowych, należy najpierw spojrzeć na wszystkie moduły z osobna. Nieważne czy mówimy tutaj o jednym urządzeniu czy jednej usłudze w konkretnym komputerze. Bezpieczeństwo ogółu równa się bezpieczeństwu najbardziej podatnego elementu. Dopiero gdy osobno wszystkie usługi, maszyny, urządzenia, magistrale i kanały komunikacyjne są zabezpieczone można przejść do spojrzenia całościowego. Jest nim bezpieczeństwo integracji wszystkich elementów. W tym momencie wkracza czynnik systemu rozproszonego i/lub przemysłowego. Wbrew pozorom z wyjątkiem, paru różnic zabezpieczanie tych systemów niewiele różni się od zabezpieczania systemu scentralizowanego. Systemy rozproszone są z reguły dużymi strukturami. Konfiguracja i wszystkie wdrożenia mogą być kłopotliwe, ponieważ elementy całej instalacji są od siebie odległe. Podstawowymi problemami i niebezpieczeństwami są protokoły komunikacyjne, które łącza wszystkie moduły systemu. Należy zwrócić uwagę na fizyczną lokalizację różnych zasobów, ponieważ nie zawsze jest ona łatwa do zdefiniowania. Systemy przemysłowe w założeniu biorą udział w procesach sterowania dużą gamą specjalistycznych urządzeń. Wyróżniają się odpowiednimi mechanizmami, które pracują w ściśle określony sposób w trudnych warunkach. Przełamanie zabezpieczeń może skutkować bardzo dużymi stratami finansowymi, zdrowiem wielu ludzi, bądź zwiększoną zawodnością sprzętu. Systemy te muszą być zabezpieczone dodatkowo pod kontem zwiększonych opóźnień, przykładowo podczas procesu zalewania nieprawidłowymi danymi. Muszą istnieć mechanizmy reagujące na czynniki zewnętrzne tj. ciepło, wilgoć lub pył – przecież zdjęcie obudowy urządzenia przez nieroztropnego użytkownika nie może skutkować lawiną nieszczęść. Trzeba też zwrócić uwagę na ataki terrorystyczne, sabotaże, bunty oraz strajki. Pomijając te aspekty, analiza bezpieczeństwa rozproszonego systemu przemysłowego niewiele różni się od bezpieczeństwa „zwykłego” systemu scentralizowanego. David Geer w artykule dla computer.org [6] przedstawił wygląd typowego, rozproszonego systemu kontroli (w przemyśle). Rysunek 3. ilustruje tą koncepcję. Analiza tego szkicu dowodzi, że wszystkie systemy – niezależnie czy są rozproszone, scentralizowane czy przemysłowe – mogą być wystawione na podobne przypadki prób przełamania zabezpieczeń. Aby przedstawić i następnie wdrożyć politykę bezpieczeństwa do przedsiębiorstwa, należy zabezpieczyć się przed wyżej wspomnianymi czynnikami. Artykuł ten nie opisuje metod zabezpieczania urządzeń końcowych (konfiguracja aktualizacji, systemów AV, usług) czy pośredniczących (firewalle, routery). W kolejnych akapitach zostanie przedstawione kilka szczególnych przepadków, na które należy zwrócić uwagę i kilka błędnych założeń, które mogą zostać przyjęte podczas procesu tworzenia systemów rozproszonych i przemysłowych. 3. Niebezpieczeństwo sieci Internet Dzisiaj coraz więcej fabryk, przedsiębiorstw oraz obiektów rządowych wymaga dostępu do Internetu. Internet jest podstawowym kanałem komunikacyjnym i zalety jego zastosowania są ogromne. Niestety Internet może zostać dodany do istniejących systemów w sposób nieroztropny, co wiąże się z bardzo ważnymi konsekwencjami. Największym błędem jest mieszanie sieci wewnętrznej firmy z częścią mającą połączenie z Internetem. W prawie każdym wypadku nie ma wytłumaczenia fakt połączenia przykładowo systemu monitoringu z dostępem do Internetu. Administratorzy często usprawiedliwiają się faktem odpowiedniej konfiguracji serwerów udostępniających usługi internetowe, ustawienia firewalli, routingiem statycznym. Internet może być źródłem tysięcy różnych ataków na system firmowy. Niezałatane usługi, opóźnienia w aktualizacjach, błędy w skryptach i aplikacjach, tzw. luki 0day umożliwiają penetrację serwerów i czasem nawet przejęcie nad nimi kontroli. Przejęty serwer może czasami służyć jako punkt startowy w dalszej penetracji systemu, rekonfiguracji routerów i urządzeń w firmie. Pospolite i niezwykle często spotykane błędy w aplikacjach desktopowych i Internetowych tj. błędy Cross Site Scripting (XSS), SQL Injection, Cross Site Request Forgery (CSRF), Arbitary File Download (AFD) i wiele innych może skutkować wyciekiem danych, ich modyfikacją lub innymi nieprzyjemnościami. Ochrona przed tymi czynnikami leży oczywiście po stronie programistów, webmasterów i innych ludzi zajmującymi się tworzeniem aplikacji dla usług sieciowych udostępnianych w firmie. Administracja samym serwerem i jego odpowiednia konfiguracja również nie jest zadaniem łatwym. Niestety wykracza ona poza tematykę tego materiału. Oprócz powyższych czynności istnieje jeden nietypowy (czasem sporny) sposób zabezpieczania sieci. Polega on na implementacji sieci Honeynet obok sieci wykorzystywanej w firmie. Honeynety są fałszywi systemami, który symulują prawdziwe sieci. Ich zadaniem jest zwabienie agresora i odciągnięcie jego percepcji od prawdziwych systemów. Agresor penetruje honeynety jak prawdziwe sieci – napotyka tam różne błędy, opóźnienia, styka się z takim samym oprogramowaniem. Ma możliwość zauważenia zarówno urządzeń końcowych – tzw. Honeypotów (np. serwerów FTP), jak i pośredniczących – np. Honeywall. Wszystko odbywa się na specjalnie przygotowanych maszynach, które nie posiadają żadnych przydatnych informacji lub w środowiskach wirtualnych. Systemy te analizują każdy ruch agresora, próbują go zidentyfikować i zlokalizować. Są przydatne w procesie schwytania agresora i dostarczenia bardzo wiarygodnych dowodów w sprawach sądowych. 4. Niebezpieczeństwo mediów transmisyjnych Medium transmisyjne jest kanałem, w którym płyną wszystkie dane pomiędzy elementami systemu rozproszonego. W najbardziej ogólny sposób media dzielą się na media przewodowe i bezprzewodowe. Medium przewodowe stwarza wiele problemów, w szczególności w systemach przemysłowych. Okablowanie przede wszystkim może zostać uszkodzone, należy więc je chronić przed dostępem osób trzecich oraz przed czynnikami zewnętrznymi. Uszkodzenie może paraliżować ruch informacji w systemie, w szczególności gdy nastąpi pomiędzy kluczowymi urządzeniami pośredniczącymi. Należy w takim wypadku stosować dodatkowe routery i przekazywać ruch innymi ścieżkami. Dostęp do medium przewodowych lub urządzeń pośredniczących stwarza niebezpieczeństwo ataków z rodzaju Man-in-the-middle. Polegają one na tym, że agresor wpina się pomiędzy dwa urządzenia przechwytując ich ruch. Przesyła ten ruch dalej nie stwarzając żadnych błędów i podejrzeń. Agresor ma wtedy możliwość podsłuchiwania tych danych lub nawet ich modyfikowanie. W celu ochrony należy stosować szyfrowanie połączeń, media optyczne (światłowody), monitorowanie sieci odnośnie przypinania nowego sprzętu. Stosunkowo nowym niebezpieczeństwem jest fakt tworzenia zakłóceń elektromagnetycznych wzdłuż kabli. Prowadzone są badania, które wskazują możliwości podsłuchiwania przesyłanych danych na podstawie analizy zakłóceń wokół kabli. Ekranowanie lub media optyczne są lekarstwem na tą właściwość. Pomimo tych negatywnych efektów powiązanych z mediami przewodowymi, nie powinno wdrażać się mediów bezprzewodowych w rozproszonych systemach przemysłowych, chyba że jest to niezbędne. Typowymi zagrożeniami mediów bezprzewodowych to przede wszystkim ryzyko podsłuchu. Ponadto należy uważać na analizę ruchu, skanowanie, kradzieże tożsamości, nieuprawniony dostęp, powtórzenia, ataki z rodziny man-in-the-middle, flooding, zniekształcanie pakietów, generowanie zakłóceń, utratę danych jak i również ataki fizyczne jak na przykład kradzież urządzeń mobilnych. Zagrożenia te spowodowane są charakterem mediów bezprzewodowych, które rozsyłają informacje w eter, do którego dostęp ma potencjalnie każdy. Ataki na ten typ medium jest niebezpieczny z powodu trudności ustalenia miejsca ataku. Podstawowym elementem polityki bezpieczeństwa powinno być stosowanie kryptografii – szyfrowania danych, stosowanie odpowiednio bezpiecznych metod uwierzytelniania, używanie protokołów retransmisji, numerowanie pakietów, używanie sesji i przypisanie do nich tokenów, stosowanie liczb pseudolosowych, znakowanie czasem transmisji i metody takie jak IPSEC. 5. Ataki rodziny Denial of Service Ataki z rodziny DoS polegają na wielokrotnej próbie dostępu do pewnego zasobu systemu. Próba ta nie musi być udana. Proces samego uwierzytelniania lub autoryzacji oraz generowane przy tym dane mogą być fundamentem ataku DoS. Ten wektor ataku jest w zasadzie typowym problemem każdej sieci i systemu komputerowego i jest szeroko opisywany w Internecie oraz w literaturze specjalistycznej, dlatego zostanie nadmieniony tutaj wyłącznie dodatkowy aspekt dotyczący systemów przemysłowych. Zalewanie pewnymi informacjami systemu rozproszonego jest bardzo łatwą metodą, która w skuteczny sposób może zachwiać pracę firmy. Systemy przemysłowe rzadko są przystosowane do tego, aby obsłużyć nadspodziewanie dużą dawkę informacji, ponieważ ich charakter pozwala przewidzieć ilość danych, które pojawiają się w każdym węźle. Efektem ataku DoS są zwiększone opóźnienia w atakowanym segmencie lub nawet całkowite jego wyłączenie podczas ataku. W systemach czasu rzeczywistego oraz w krytycznych urządzeniach zakładu pracy – minimalne zwiększenie opóźnień może powodować duże straty finansowe lub inne nieprzyjemności. Poza typowym atakiem DoS, nowo obserwowanym niebezpieczeństwem jest tzw. FDoS czyli Financial DoS. To nowe niebezpieczeństwo, które można zaobserwować w firmach silnie związanych z Nowymi Technologiami, które decydują się na rozwiązania Cloud Computing w stylu Iaas (Infrastructure as a Service). Iaas to zbiór komputerów znajdujących się w Data Centre na których uruchamiane są wirtualne maszyny. Maszyny te są dzierżawione. Opłaty naliczane są za wykorzystaną moc obliczeniową i firma rozliczana jest w zasadzie za każdą wykonaną operację. Są to rozwiązanie niezwykle skalowalne i od kilku lat cieszy się wielkim zainteresowaniem. Financial DoS jest atakiem łatwym w przeprowadzeniu. Polega na wielokrotnym pobieraniu wskazanego zasobu znajdującego się w chmurze. Dziecinnie proste skrypty mogą wygenerować duży ruch między przedsiębiorstwem a chmurą co będzie implikować naliczanie opłat za korzystanie z usług Cloud Computing. Może się okazać, że po kilku dniach firma może nawet zbankrutować. Ataki DoS są atakami najprostszymi do przeprowadzenia, a w zasadzie najtrudniejszymi do obrony. Gdy agresor posiada odpowiednie zasoby, jest w stanie odciąć kluczowe węzły systemu na wiele dni. Rygorystyczna polityka bezpieczeństwa na Firewallach, korzystanie z Load Balancerów i implementacja modułów wykrywających zalewanie w prawie każdym węźle systemu to kluczem do sukcesu zapobiegania tego ataku. Najważniejsze jednak jest przystosowanie sprzętu i oprogramowania do faktu wystąpienie ataku DoS w taki sposób, aby jego ewentualne skutki były jak najmniej dotkliwe. 6. Ataki na metody uwierzytelniania i autoryzacji Każdy system musi udostępniać procesy uwierzytelniania i autoryzacji. Sprawdzenie wiarygodności dokumentów, plików oraz innych obiektów nazywa się procesem uwierzytelnienia. Mechanizm autoryzacji sprawdza jakie uprawnienia posiada obiekt (np. Jan Kowalski) do innych obiektów w systemie (np. do sterowników zaworów w ciepłowni czy plików na dyskach sieciowych). Dwa te procesy można zauważyć codziennie praktycznie na każdym kroku. Serwisy internetowe wymagają loginów i haseł, bankomaty i telefony komórkowe – kodów PIN, pani na poczcie podczas odbierania paczki – dokumentu ze zdjęciem oraz podpisu. Pomimo że procedury te wydają się być mało skomplikowane, istnieje wiele niebezpieczeństw związanych z uwierzytelnianiem oraz autoryzacją. Autoryzacja i uwierzytelnianie są dwoma podstawowym elementami, które chronią zasoby przed niepowołanym dostępem. Wdrożenie tych mechanizmów jest dla administracji dość intuicyjne. Niestety – wdrożenia tych mechanizmów są ogółem nieprzemyślane. Zwiększa się wtedy podatność całego systemu na bardzo poważne i łatwe w przeprowadzeniu ataki. Administracja w dodatku nie monitoruje poczynań, sądząc, że zrobiono już wystarczająco dużo. Przecież, jeśli nawet w systemie coś się wydarzy – to dokładnie będzie wiadomo kto jest za to odpowiedzialny. Nic bardziej mylnego! Gorsze od braku bezpieczeństwa jest tylko złudne poczucie bezpieczeństwa. Przez takie uchybienia można być świadkiem najbardziej szkodliwych ataków, które polegają na łamaniu prostych, słownikowych haseł lub logowania do różnych systemów na domyślnie ustawione hasła dostępowe. Jaką obrać strategię, jeśli chodzi o weryfikację danych? Przeanalizujmy poniższe rozwiązania: hasła dostępowe karty magnetyczne karty inteligentne Hasła dostępowe to ciągi znaków, które są zazwyczaj zapamiętywane przez użytkowników. Są najprostsze w drożeniu, jednak posiadają bardzo dużo wad. Podstawowa słabość to możliwość stosowania wydajnych ataków siłowych (Bruteforce) lub słownikowych (Dictionary Based Attacks). Gdy hasło jest krótkie lub mało skomplikowane, można wiele razy próbować „zgadnąć” odpowiedni ciąg znaków w procesie uwierzytelniania. Automatyzacja tego procesu może być czasem skomplikowana (trudny dostęp agresora do terminali lub urządzeń w systemach przemysłowych oraz stosowanie mniej typowego sprzętu niż w klasycznych systemach informatycznych), jednak nie można lekceważyć tego typu ataków. Hasła dostępowe powinny być podane na wejście kryptograficznych bezpiecznych funkcji skrótu. Wynikiem ich stosowania, jest zapisywanie w bazie hasła w sposób niejawny, za pomocą „hashu” – który nie może zostać zamieniony w podane wcześniej hasło (proces zamiany hasła na hash jest procesem jednokierunkowym). Niezastosowanie się do czynności „hashowania” haseł może mieć tragiczne skutki. Hasła użytkowników mogą być przeglądane przez administracje, nielegalnie używane lub przekazywane dalej. Brak „hashowania” powiększa szkody wynikłe z przełamania zabezpieczeń. Agresor pobierając bazy danych ma dostęp bezpośrednio do tych danych i może je wykorzystywać do dalszej penetracji systemu. Warto zaznaczyć, że „hashowanie” nie rozwiązuje wszystkich problemów. Bezpieczne funkcje hashujące to stosunkowo wąska grupa funkcji skrótu kryptograficznego. Algorytmy większości z nich są dobrze znane i istnieje wiele narzędzi, które potrafią łamać takie „bezpieczne” hasła. Najpopularniejszym atakiem na hasła poddane działaniu funkcji skrótu jest stosowanie tęczowych tablic (Rainbow Tables). Są to ogromne zbiory danych, które bardzo przyśpieszają proces łamania. Stosowanie przetwarzania równoległego, wykorzystanie rdzeni wielu procesorów CPU oraz GPU coraz bardziej przyśpiesza tą procedurę. Hasła poniżej 12 znaków mogą być łamane na najszybszych komputerach w kilka godzin, co jest wynikiem bardzo niepokojącym (w domowych warunkach łamanie haseł 8-9 znakowych jest również wykonalne w rozsądnym czasie). Lekarstwem na to jest stosowanie długich haseł, dodawanie tzw. soli (salt), korzystanie z kilku funkcji hashujących (zmniejszanie problemu tzw. kolizji i ataków urodzinowych). Dodatkowo hasła mogą zostać bardzo łatwo podsłuchane. Należy dobrze chronić medium transmisyjne i prawidłowo konfigurować wszystkie urządzenia pośredniczące i końcowe. Należy uważać na aspekt ludzki – przekazywanie haseł, zapisywanie ich, zapamiętywanie w różnych aplikacjach. Karty magnetyczne zawierają jednoznaczny identyfikator użytkownika oraz pewien certyfikat (wystawiany np. za pomocą algorytmu RSA). To rozwiązanie jest dużo bezpieczniejsze niż stosowanie haseł dostępowych. Przede wszystkim przeciwdziała się tworzeniu „nowych” tożsamości, ponieważ karta generowana jest przez serwer (administrację) dla danego użytkownika i mu przekazywana. Podjęcie próby autoryzacji jest również mocno utrudnione, ponieważ agresor musi zdobyć kartę magnetyczną lub sam ją skonstruować (wraz z przechwyconymi wcześniej certyfikatami i innymi danymi). Uwierzytelnianie powinno być rozszerzone o stosowanie dodatkowych haseł (przykładowo haseł PIN). Dzięki temu istnieje możliwość, że skradziona karta nie wyrządzi wielu szkód w organizacji. Karty magnetyczne niestety mogą być łatwo podsłuchiwane oraz kopiowane. Pracownicy muszą bardzo uważać na sposób ich przechowywania i pod żadnym pozorem nie mogą pożyczać swoich kart innym pracownikom. Należy wdrożyć też odpowiednie procedury, które będą stosowane w momencie zgubienia lub zniszczenia karty. Karty inteligentne są kartami, które posiadają mikroprocesor, pamięć RAM, ROM, EEPROM, czasem nawet koprocesor kryptograficzny. Oprócz uwierzytelnia, stosowane są też m.in. w płatnościach lub w technologiach GSM. Główną ich zaletą jest duża odporność na kopiowanie, modyfikacje fizyczne oraz podsłuch. Prawidłowe wdrożenie ich do systemu może być kosztowne i wymaga dokładnego przemyślenia procesu uwierzytelniania – chociażby analizy algorytmów stosowanych w samej karcie inteligentnej. Problemy natury ludzkiej są podobne jak w przypadku kart magnetycznych. Z powyższej analizy można wyciągnąć wiele wniosków. Każde rozwiązanie niesie ze sobą pewne zalety i wady. Najlepszym wyjściem jest stosowanie kilku metod oraz dogłębne zabezpieczenie ze strony ataków socjotechnicznych, czysto fizycznych (szantaże, kradzieże, manipulacje). Personel musi być wyszkolony, znać wektory ataków oraz ich skutki. Dr. Piotr Szpryngier [7] przedstawił wszystkie problemy z wyżej podanymi metodami w postaci tabeli i ocenił ich poziom bezpieczeństwa. Tabela 1. pokazuje jak kryptografia może podnieść poziom bezpieczeństwa. Stosowanie algorytmów asymetrycznych pozwala zabezpieczyć dane przed ujawnieniem. Stosowanie haseł generowanych dynamicznie (wykorzystanie np. haseł jednorazowych) pozwala tanim kosztem stworzyć solidne sposoby uwierzytelnienia. W kluczowych strefach systemu rozproszonego największą ochronę zapewniają karty inteligentne, w szczególności te, które używają algorytmów opartych na Dowodzie o Wiedzy Zerowej. Jest to własność, w której jedna ze stron potrafi drugiej stronie udowodnić, że posiada pewną informację, bez jej ujawniania. Protokół Odgadnięcie Podsłuch Podszywanie Ujawnienie sekretu Poziom bezpieczeństwa Hasła – – – – 1 Karty magnetyczne (symetryczne) + – – – 2 Karty magnetyczne (asymetryczne) + – – + 3 Hasła dynamiczne + + – – 4 Karty inteligentne (symetryczne) + + – – 4 Karty inteligentne (asymetryczne) + + + + 5 Karty inteligentne (DWZ) + + + + 6 Tabela 1 : Bezpieczeństwo protokołów uwierzytelniania Po pomyślnym wdrożeniu metod uwierzytelniania, autoryzacja może wydawać się prostym mechanizmem. Wystarczy przypisać uwierzytelnionym obiektom prawa dostępu do innych obiektów w systemie. Pomijając już same aspekty nadawania uprawnień, należy pamiętać o kilku ważnych kwestiach. Przede wszystkim należy zabezpieczać magazyny danych tymczasowych. Są one prawe zawsze niezbędne do prawidłowego działania systemu. Wysyłane dane (np. pliki wysyłane przez aplikacje klienckie) w początkowej fazie są odbierane przez serwery i umieszczane w przestrzeni, do której dostęp ma w zasadzie każdy użytkownik (domyślnie). Pomimo stosowania różnych metod filtracji danych należy pamiętać – że dane, aby mogły być przeanalizowane przez system zabezpieczający – muszą najpierw fizycznie się w nim znaleźć. Takie dane tymczasowe mogą być odczytane i wykonane po stronie serwera zanim jeszcze filtr zdąży je przeanalizować i usunąć. Przykładowym atakiem na system autoryzacji może być proces przesyłania plików do serwera (np. poprzez protokół HTTP). Odpowiednie skrypty po stronie serwera są odpowiedzialne za analizę przysłanego pliku i w wypadku wykrycia dozwolonego formatu, przenoszą plik z przestrzeni plików tymczasowych do odpowiedniej lokalizacji. Agresor, gdy ma dostęp do systemu (jako np. jego zwykły jego użytkownik) prawdopodobnie ma możliwość uruchamiania pewnej grupy aplikacji, które również generują dane tymczasowe. Dane te trafiają do odpowiedniego (wspólnego) katalogu i istnieje możliwość zidentyfikowania ścieżki do tego folderu. Znając ścieżkę, agresor jest w stanie zlokalizować wszystkie pliki tymczasowe. Jako że, katalog tymczasowy musi mieć odpowiednio niskie wymagania odnośnie zabezpieczeń, agresor może przesłać złośliwy kod poprzez omawiany wyżej system i w momencie, gdy plik będzie analizowany przez różne filtry – wykonać go po stronie serwera. Plik zostanie odrzucony przez filtry, jednak złośliwy kod zostanie wykonany. Powyższy przykład pokazuje, że zasady autoryzacji powinny być ustawiane bezpośrednio na każdy obiekt z osobna. Strefy, do których dostęp ma kilka obiektów jednocześnie) lub kilka różnych typów obiektów) – powinny być w miarę możliwości jak najbardziej od siebie odseparowane (poprzez piaskownice, wirtualizacje, „ jaile ” lub przez zwyczajną zmianę lokalizacji). Uwierzytelnianie oraz autoryzacja są bardzo ważnymi elementami w każdym systemie i muszą być dokładnie chronione. W rozproszonych systemach przemysłowych jest to bardzo ważne, ponieważ procesy te są wykorzystywane bardzo często, na bardzo wielu obiektach, bardzo od siebie odległych. Nie ma możliwości zastąpienia maszyn poprzez ludzi i należy polegać na ich niezawodności. Należy jednak ją zapewnić oraz odpowiednio przewidzieć wszystkie sytuacje wyjątkowe, jakie mogą wystąpić. Błędy projektowe popełnione w tym aspekcie narażają całe przedsiębiorstwo na straszne niebezpieczeństwo. 7. Ataki socjotechniczne. Błędy ideologiczne i filozoficzne. Wiele systemów może być zabezpieczonych bez ingerencji w technologie. Zgodnie z badaniami J. Schopmana [2] podstawą ryzyka w IT nie są techniczne implementacje kolejnych mechanizmów tylko ideologie dzięki którym wytwarza się te technologie. Dlatego rozwiązania informatyczne powinny być używane tylko w ściśle określonych warunkach. Rozproszone systemy, których głównym zadaniem jest zapewnienie podstawowych mechanizmów do działania szerokiej gamy aplikacji i urządzeń we wszystkich filiach firm są więc z definicji narażone na duże niebezpieczeństwo. Analiza zachowań społecznych oraz ludzkiej psychiki pomaga zwiększyć zabezpieczenia często dużo skuteczniej, niż skomplikowane oskryptowanie serwerów. Ideologia zabezpieczania systemów komputerowych różni się od ideologii etyki hakerskiej. Aby poprawnie wdrożyć czynności zmniejszające szanse udanego ataku, należy przeanalizować dokładnie największe różnice w psychice administratora systemu i agresora (hakera). Typowa społeczność hakerska uznaje kult wolności informacji. Hakerzy uważają, że informacja powinna być powszechnie znana, wykorzystywana w dalszych celach przez wielu ludzi. Utopijna wartość tego kultu ma za zadanie tworzenie lepszego świata, zapewniającego szybszy rozwój technologiczny, społeczny i artystyczny, poprzez analizę rzeczy już stworzonych. Rzeczy, które można poprawić, ulepszyć, na podstawie których można wysnuć przyczyny sukcesów i porażek. Zbiorowość zajmująca się zabezpieczaniem informacji, zazwyczaj wyznaje skrajnie odmienny pogląd w tej dziedzinie. Administratorzy zajmują się ograniczaniem dostępu do informacji, dyktują prawa i przepisy. Nie pozwalają osobom obcym korzystać z wyprodukowanych dóbr (materialnych i intelektualnych). Kapitalistyczna wartość tego podejścia ma na celu stworzenie grup społecznych, których wartością jest posiadanie jak największej liczby dóbr w stosunku do konkurencji. Czym gorszymi narzędziami będzie dysponował konkurent i czym mniejszą ilością potrzebnych informacji będzie zarządzał – tym ta grupa będzie czuła się lepiej. Stereotypowy administrator postrzega hakera jako osobę agresywną, niemoralną, siejącą spustoszenie. Jest to przestępca, który dla rozrywki lub poprzez nielegalne działanie konkurencji chce zakłócić spokój firmy. Typowy haker patrzy na administratora systemu jak na okrutnego dyktatora stawiającego irracjonalne wymogi i ograniczenia. Powyższa analiza już nasuwa podstawowe wnioski. Po pierwsze administrator, nie powinien stawiać zbyt wielu przeszkód typowym użytkownikom. Każdy system jest stworzony przez ludzi, dla ludzi. Dużym błędem jest stworzenie systemu, gdzie użytkownik nie może komfortowo pracować, poprzez nałożenie zbyt wielu ograniczeń i poświadczeń. Ogromna ilość regulacji denerwuje typowego hakera, który będzie próbował złamać zabezpieczenia. Agresor pokaże nieskuteczność całego stworzonego procesu i uwolni wielu ludzi od cierpień dyktatora. Administrator przede wszystkim musi tak zarządzać rozproszonym systemem w przemyśle – aby zwiększać efektywność pracowników, a nie tworzyć fortecy nie do zdobycia (oraz takiej, w której nie można pracować). Błąd nadmiernego ograniczania zasobów bardzo często przenosi się na inne płaszczyzny, między innymi na płaszczyznę nadmiernej kontroli. Społeczny problem tego rodzaju występuje nagminnie w wielu firmach. Zgodnie z ankietą przeprowadzoną w magazynie MacWorld [3], 21.6% korporacji przeszukiwało pliki swoich pracowników. W 66.2% przypadków pracownicy nie byli o tym odpowiednio uprzedzani. Monitoring, wynajmowanie firm detektywistycznych i stosowanie innych tego typu metod podważa moralność ludzi zabezpieczających systemy. Podobnie jak w powyższym przypadku, może to zwrócić się przeciwko nim. Monitorowanie plików, urządzeń, pracowników dla sprawnego hakera jest łatwe do wykrycia. Pomijając już ekstremalne przypadki przełamania zabezpieczeń urządzeń lub magistral audio/wideo, możliwe jest łatwe zmanipulowanie administracji. Ułatwia to tworzenie pewnych hałaśliwych lub nietypowych zachowań (celowa kłótnia, wypadek, znalezienie się w nieodpowiedniej strefie, zasłabnięcie) aby zogniskować system monitoringu i percepcję ochrony na konkretne wydarzenie. Podczas gdy przewrażliwiony „Wielki Brat” będzie przyglądał się jednemu pracownikowi, drugi może zaatakować system poprzez instalację szkodliwego oprogramowania lub ingerować w warstwę fizyczną systemu (sprzętowe keylogery, podłączanie się kablami konsolowymi do routerów, kradzież lub niszczenie sprzętu). Kolejnym problemem jaki może przysporzyć wielu problemów jest Log Flooding, czyli generowanie zbyt dużej ilości ostrzeżeń o zbyt niskim priorytecie. Wiele zabezpieczeń wytwarza dużo informacji, które po pierwsze mogą zakłócić poprawne działanie sieci przemysłowych, a po drugie mogą powodować, że ważne informacje zostaną zagubione. Stwarza to bardzo złudne poczucie bezpieczeństwa – chociaż systemy są dobrze chronione, skuteczność administracji i audytów spada co implikuje drastyczne załamanie poziomu bezpieczeństwa. Wiele razy różne organizacje zajmujące się zawodowo audytami bezpieczeństwa analizowały profil agresora. Zgodnie z badaniami E.R. Bucka oraz T. Forestera [4][5] typowy człowiek łamiący prawo poprzez systemy IT jest w wieku 18-46 lat, jest silnie zmotywowany, rządny wyzwań, energetyczny, bystry i uśmiechnięty. Przestępcy komputerowi (nie koniecznie hakerzy!) zazwyczaj zdają się być oddanymi pracownikami, mają dobre stosunki z przełożonymi i nie zawsze mogą pochwalić się ponadprzeciętną wiedzą z zakresu technologii informatycznych. Motywują ich problemy finansowe, alkohol lub narkotyki. Najczęstszą przyczyną łamania zabezpieczeń, kradzieży danych lub dewastacji sprzętu jest frustracja otoczeniem, denerwującymi nawykami współpracowników lub ograniczeniami systemu. Ludzi takich tych trudno nazwać hakerami, jednak to przez nich zostaje przełamane najwięcej zabezpieczeń. Rozproszenie systemów w przemyśle dodatkowo wzburza negatywne emocje, ponieważ dostęp do sprzętu znajdującego się w wielu lokalizacjach jest łatwiejszy, kontrole są mniejsze. Administratorzy zazwyczaj nastawiają swoje systemy przeciwko stereotypowym hakerom – którzy stanowią najmniejszą liczbę „włamań”. Stereotypowy haker to dziecko szczęścia z ponadczasowym umysłem i umiejętnościami. Przeprowadzone badania naukowe pozwoliły zidentyfikować prawdziwe dążenia największych przestępców komputerowych. Czynniki te zostały nazwane przez J.M. Carrolla [10] mianem modelu MOMM – Motive, Opportunity, Means and Method. Hakerzy są egoistami, którzy chcą pokazać swoją wyższość intelektualną poprzez zademonstrowanie przełamania zabezpieczeń systemów, sabotowanie ich i chwalenie się zdobytymi informacjami w odpowiednich kręgach – czasem na tym zarabiając. Jest to całkowicie inne podejście niż do cudownego dziecka z laptopem rodziców na kolanach lub do rycerzy, którzy nigdy nie szkodzą systemom i nigdy nie mają z tego powodu korzyści materialnych. Prawidłowym podejściem administracji podczas wdrażania polityki bezpieczeństwa jest zrozumienie faktu, że włamywaczem może być dosłownie każdy. Aby prawidłowo określić zasady bezpieczeństwa, administrator systemu musi dokładne przestudiowanie cztery czynniki polityki bezpieczeństwa: Precyzyjność – zasady pracy oraz ograniczeń muszą być jasno określone – zarówno ich przyczyny oraz ograniczenia. Tworzone reguły muszą być jednoznaczne i zrozumiane przez każdego pracownika. Prywatność – czyli jaka informacja odnośnie jakiej osoby lub grupy osób powinna być dostępna tylko i wyłącznie dla tych osób. Kto jest odpowiedzialny za przydzielanie dostępu do zasobów, kto może mieć dostęp do błędów, kto może reagować na zdarzenia. Reguły powinny być przeanalizowane pod kątem odpowiedzi jednoznacznie na pytanie: którzy ludzie, jaką grupę informacji muszą mieć tylko w swoim zasięgu. Własność – kto przetrzymuje informacje? W jakiej formie? Kto kontroluje kanały transmisji? Kto odpowiada za media transmisyjne, magistrale, linie kolejowe, światłowody – którymi informacja może płynąć? Jaki jest dostęp do tych mediów? Dostępność – jaką informację, jaka osoba posiada, komu może ją przekazać, pod jakimi warunkami i w jakiej formie. Czy istnieje możliwość delegowania pewnych zadań? Kogo można prosić o pomoc oraz komu można pomagać i w jakich sprawach. Każdy reguła musi dokładnie precyzować te cztery czynniki. Aby zminimalizować ryzyko ataku socjotechnicznego, wycieku informacji, zepsucia sprzętu lub innych wypadków należy zawsze brać pod uwagę skrajne przypadki: przypadek ataku hakera na wielu poziomach abstrakcji oraz rolę zwyczajnego pracownika lub maszyny, która na co dzień musi w efektywny sposób spełniać swoje zadanie. Oczywiście, nie zapominajmy o zwyczajnym, legalnym użytkowniku. Przedstawione wyżej wskazówki są na tyle ogólne, że trudno wdrożyć je w życie. Aby to zmienić można zastosować sprawdzone frameworków używanych przez audytorów bezpieczeństwa. Zbiór takich gotowych reguł powinien uwzględniać naturalne zachowania ludzkie, powinien być skalowalny i powinien być jak najmniej zależny od technologii czy strategii firmy. Jednym z takich systemów jest rozszerzony model ISSI (Extended ISSI Model). Bardzo dobrze funkcjonuje od we wszelakich systemach rozproszonych i je rozszerza o nowe poziomy abstrakcji. Sztandarowym przykładem jest rozszerzanie modelu OSI połączeń sieciowych. Transmisja danych jest głównym zadaniem rozproszonych systemów przemysłowych, dlatego dalej zostanie ta czynność poszerzona o nowe elementy. Techniczne problemy zabezpieczenia transmisji są opisane w modelu OSI. Model OSI (zastępowany przez model TCP/IP – ale w tym przypadku nie ma to znaczenia) podaje sposoby ochrony danych i reakcji systemu na pewne sytuacje w wielu aspektach. Uszeregowanie pewnych czynności w niezależne od siebie grupy pozwala na łatwe określenie wymagań odnośnie bezpieczeństwa. Gdy zależy nam na poufności, wiemy że powinniśmy w warstwie fizycznej używać przykładowo światłowodów (odpornych na podsłuchy, które bardzo trudno podmienić). Wiemy, jakie reguły stosować w sieci lokalnej (warstwa druga) oraz w sieciach rozległych/rozproszonych (warstwa trzecia) i tak dalej. Schemat 1. ilustruje jakie powinno być podejście administratora podczas generowania zbioru reguł, które powinny być ustanowione aby informacja była bezpiecznie przesłana z punktu A do punktu B. Uwzględnia to proces technologiczny, zarządzanie informacją, administrowanie strukturą, aspekt prawny oraz dodatkowo uwzględnić zachowania społeczne. Każda warstwa musi działać niezależnie od innej i rozwiązywać w jednoznaczny sposób tylko jeden problem. Dodatkowy komentarz jest w zasadzie zbędny, schemat przedstawia dokładnie tą ideologię, dodatkowe informacje można znaleźć w artykułach IEEE [1]. Powyższe przykłady i wskazówki pokazują bardzo duży wpływ czynnika ludzkiego na niebezpieczeństwo Rozproszonych Systemów Przemysłowych. Warto uzmysłowić sobie fakt, że jest to najważniejsze zagrożenie praktycznie dla każdego systemu. Systemy rozproszone są stworzone do zarabiania dużej ilości pieniędzy, są administrowane przez wiele osób i zarządzają pracą tysięcy pracowników. Dużo trudniej zabezpieczyć jeden serwer WWW w konkretnym pokoju, niż farmę czujników rozsianych po tysiącach urządzeń w kilku miastach. Dodatkowo warto zaznaczyć, że systemy przemysłowe są dużo bardziej narażone na ataki z przyczyny użycia konkretnych, wyspecjalizowanych technologii. Wymagany jest więc wywiad, szpiegowanie, wydobywanie informacji od wewnątrz, aby dowiedzieć się jak te technologie wyglądają. 8.Źródła [1] Jussipekka Leiwo (Monash University), Seppo Heikkuri (Nokia Telecommunications). „An Analysis of Ethics as Foundation of Information Security in Distributed Systems” [2] J. Schopman.„Information technology’s ideology makes its user risky”. Facing the Challenge of Risk and Vulnerability in an Information Society [3] C. Piller. „Bosses with x-ray eyes”. MacWorld, July 1993 [4] E.R. Buck. „Introduction to Data Security and Controls”. QED Technical Publishing Group, 2nd edition, 1991 [5] T. Forester and P. Morrison. „Computer Ethics, Cautionary Tales and Ethical Dilemmas in Computing.” Basil Blackwell, Ltd. 1990 [6] David Geer „Security of Critical Control Systems Sparks Concern”, computer.org, IEEE Computer Society, 2006 [7] dr inż. Piotr Szpryngier „Bezpieczeństwo Systemów Rozproszonych”, skypt [8] prof. dr. hab. inż. Andrzej Kwiecień „Analiza przepływu informacji w komputerowych sieciach przemysłowych”, Politechnika Śląska, 2000 [9] Cezary Sobaniec, Jerzy Brzeziński „Wykład Systemy rozproszone – Wprowadzenie” Uczelnia Online [10] J.M. Carroll “A portrait of the computer criminal”. In IFIP TC11 11th International Conference of Information Systems Security, 1995 [11] Wiedza ogólna, Internet, Google i Wikipedia ,Mr. Niebezpieczny